{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piai/anaconda3/envs/course/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim # version 4\n",
    "# conda install -c anaconda gensim\n",
    "# pip install gensim==4.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: prepare the corpus for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 주어진 data로 gensim을 활용하여 word2vec 모델 학습\n",
    "\n",
    "# 학습을 위한 데이터 로딩 -- Data 준비\n",
    "class TextIterator(object):\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in open(self.fname,encoding='utf-8'):\n",
    "            yield line.split()\n",
    "\n",
    "filename = 'newskor.txt'\n",
    "sentences = TextIterator(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2, 3: Training & Load Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "train = True # train flag (True: train model / False: load trained model)\n",
    "SIZE = 300 # vector size\n",
    "WINDOW = 5 # context window 앞뒤로  5+target+5\n",
    "SG = 1 # 1 == skip-gram / 0 == cbow\n",
    "MIN_COUNT = 10 # ignores all words appearing lower than min_count #적게 등장하는 단어는 굳이 학습을 하지 않도록 함.\n",
    "WORKERS = 20 # cpu cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    model = gensim.models.Word2Vec(\n",
    "        vector_size=SIZE, window=WINDOW, sg=SG, \n",
    "        min_count=MIN_COUNT, workers=WORKERS\n",
    "    )\n",
    "    model.build_vocab(sentences) # prepare model vocab\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=1)\n",
    "    model.save('newskor.model')\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load('newskor.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 하\n",
      "1: 이\n",
      "2: .\n",
      "3: 는\n",
      "4: 을\n",
      "5: ㄴ\n",
      "6: 다\n",
      "7: 의\n",
      "8: 에\n",
      "9: 를\n",
      "10: 은\n",
      "11: 어\n",
      "12: 있\n",
      "13: 고\n",
      "14: 으로\n",
      "15: 가\n",
      "16: 였\n",
      "17: ㄹ\n",
      "18: 되\n",
      "19: ,\n",
      "20: 에서\n",
      "21: 었\n",
      "22: )\n",
      "23: (\n",
      "24: 로\n",
      "25: 것\n",
      "26: 도\n",
      "27: 등\n",
      "28: 과\n",
      "29: 들\n",
      "30: 지\n"
     ]
    }
   ],
   "source": [
    "vocab = model.wv.index_to_key # See vocabs\n",
    "for i, v in enumerate(vocab):\n",
    "    print(\"{}: {}\".format(i, v))\n",
    "    if i==30: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06196966  0.2443025  -0.00139966  0.02323417 -0.28431296 -0.18615228\n",
      " -0.03106055  0.09346455 -0.03404297 -0.02171713 -0.08730045 -0.03931028\n",
      " -0.2926308   0.07602247  0.07774882  0.12796213  0.00265447 -0.12028006\n",
      "  0.10675357 -0.09822863  0.02753089 -0.27141872 -0.14659564 -0.00650539\n",
      " -0.07444834 -0.08077253 -0.13549362  0.11622631 -0.11447094  0.05133944\n",
      " -0.05963147 -0.04824286  0.19740534  0.09139168 -0.12114628 -0.02173519\n",
      "  0.06208365 -0.18713741 -0.1348079   0.08764952 -0.05121287 -0.05168116\n",
      "  0.24995574  0.18817267  0.12913667  0.13066295 -0.25786987  0.20005894\n",
      "  0.42409542  0.18157567  0.09399304  0.31929448 -0.01467193  0.08827894\n",
      "  0.2192231   0.35404733 -0.07174036  0.12566915 -0.05472526 -0.02516148\n",
      " -0.08312676 -0.0346135  -0.13047582  0.02981478 -0.06950638  0.20200075\n",
      "  0.16968367  0.03333197 -0.08733575  0.19859171 -0.07983916  0.26091725\n",
      " -0.17213994 -0.30112422  0.19590712  0.11933541  0.12719485 -0.16143684\n",
      " -0.03746552 -0.05472356 -0.00304054 -0.15337983  0.02250536  0.2889538\n",
      "  0.02564798 -0.14799795 -0.23136568  0.04890598  0.16961426  0.43060076\n",
      "  0.11945688 -0.21350199 -0.21877794 -0.07368292  0.17370103  0.0786355\n",
      "  0.12770899 -0.05118901 -0.25617158  0.07697453  0.08684759  0.13112734\n",
      " -0.0365226   0.30264223  0.0907395  -0.15724221 -0.02582278  0.35408574\n",
      " -0.29008555  0.04469453 -0.09461872 -0.23262174  0.14204994  0.06879105\n",
      "  0.10980228  0.02084943  0.04186153  0.07184278 -0.02040045  0.02997451\n",
      "  0.06751991 -0.15632607  0.09183104 -0.13986944 -0.12839542  0.08808167\n",
      " -0.09228247 -0.19674535 -0.01798667 -0.04356711  0.09774316  0.10897032\n",
      " -0.02273657 -0.1905417   0.2158117   0.26419044  0.20287813 -0.11392342\n",
      " -0.16773562 -0.02032316 -0.09117872 -0.26338097  0.01301858 -0.05448372\n",
      " -0.09394264  0.10486494 -0.05228356  0.00129752 -0.1266477  -0.2116529\n",
      " -0.01812113 -0.08533482 -0.19355945 -0.06800447  0.03498111  0.07524888\n",
      "  0.06566831  0.13094877 -0.10863224 -0.05267997 -0.05557456  0.14723153\n",
      " -0.02730468 -0.07309285 -0.07266856  0.01052697 -0.10833693 -0.25756896\n",
      "  0.08393945  0.07592293  0.0842872  -0.12052974  0.10952786 -0.06351274\n",
      "  0.12637049 -0.00484023 -0.1005773  -0.05469505 -0.10731842  0.07644752\n",
      "  0.02821509  0.11516214  0.02838846 -0.2010472  -0.05778098  0.02847406\n",
      "  0.10905628  0.02620836  0.13738261  0.01582687 -0.00858006 -0.09316006\n",
      " -0.00501859  0.10524732  0.1424921  -0.2652309   0.14714853 -0.13905212\n",
      " -0.05578606  0.01669709  0.07046872 -0.28705773  0.12711583  0.09980619\n",
      "  0.10430563 -0.00339469  0.02181082  0.06976634  0.08828752  0.10172638\n",
      " -0.18943533 -0.27589    -0.09075668 -0.13281284  0.0547784  -0.29280692\n",
      "  0.00870355 -0.1276122   0.06048864  0.09999315 -0.02257049 -0.00791933\n",
      "  0.05930831 -0.01614786  0.07044515  0.05351375  0.00472305 -0.08319083\n",
      " -0.2371543   0.16454239  0.08453708 -0.26844823  0.02834986 -0.01743962\n",
      " -0.24086784  0.04140933  0.08951926 -0.05355987 -0.01944326 -0.19455236\n",
      " -0.06538887 -0.14024565  0.09299335  0.0112196   0.25156066  0.05482867\n",
      " -0.1377768  -0.10188164  0.08716127  0.16755737 -0.13402632  0.20283513\n",
      "  0.12501842  0.02252288 -0.21321104 -0.00507174  0.01182461  0.09512533\n",
      " -0.10013144 -0.10840886  0.09173734  0.14177087 -0.02149607 -0.07742023\n",
      " -0.13303684  0.21113501  0.1877163   0.06335407 -0.18058392  0.3025199\n",
      " -0.103031    0.10183072  0.15571699 -0.09296476  0.04824711  0.02757623\n",
      "  0.30091515  0.23190919 -0.30435023  0.16042815  0.00322773  0.24844621\n",
      "  0.05152976  0.27161902 -0.03878025 -0.18591593 -0.09177386  0.16509317\n",
      "  0.13690257  0.1129851   0.04188164  0.17774019  0.22988456  0.15262263\n",
      "  0.2380362   0.19470373  0.0569666  -0.05494893  0.40803134 -0.00163058]\n",
      "size of vector:  300\n"
     ]
    }
   ],
   "source": [
    "## check word embed result\n",
    "word = '버스'\n",
    "print(model.wv[word])\n",
    "print('size of vector: ', len(model.wv[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4: Get word similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caculate the similarity between word 1 and word2\n",
      "word1: 컴퓨터\n",
      "word2: 치약\n",
      "the similarity between 컴퓨터 and 치약 :  0.5219859\n"
     ]
    }
   ],
   "source": [
    "#word1 = '한국'\n",
    "#word2 = '북한'\n",
    "print (\"Caculate the similarity between word 1 and word2\")\n",
    "word1 = input(\"word1: \")\n",
    "word2 = input(\"word2: \")\n",
    "\n",
    "# check the words are in the vocabulary\n",
    "no_problem = True\n",
    "vocab = model.wv.index_to_key\n",
    "\n",
    "if word1 not in vocab:\n",
    "    print ('the word ' + word1 + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "\n",
    "if word2 not in vocab:\n",
    "    print ('the word ' + word2 + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "\n",
    "if no_problem:\n",
    "    similarity = model.wv.similarity(word1, word2)\n",
    "    print ('the similarity between ' + word1 + ' and ' + word2 + ' : ', similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Find mismatch word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find mismatched word in the words\n",
      "text(words): 한국 북한 김밥\n",
      "the mismatch word between 한국 북한 김밥 is 김밥\n"
     ]
    }
   ],
   "source": [
    "#words = '소프트웨어 네트워크 프로그램 가방'\n",
    "print(\"Find mismatched word in the words\")\n",
    "text = input(\"text(words): \")\n",
    "words = text.split()\n",
    "\n",
    "# check the words are in the vocabulary\n",
    "no_problem = True\n",
    "vocab = model.wv.index_to_key\n",
    "\n",
    "for word in words:\n",
    "    if word not in vocab:\n",
    "        print('the word ' + word + ' is not in the vocabulary')\n",
    "        no_problem = False\n",
    "        break;\n",
    "\n",
    "if no_problem:\n",
    "    mismatched = model.wv.doesnt_match(words)\n",
    "    print ('the mismatch word between ' + text +' is', mismatched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Find the top-N most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print the most similar words\n",
      "word: 음식\n",
      "[('옷', 0.8326328992843628), ('맵', 0.8317863345146179), ('빵', 0.8258019089698792), ('맛', 0.8165563344955444), ('아이들', 0.8155353665351868), ('화장실', 0.8116585612297058), ('습관', 0.8098694682121277), ('과일', 0.8042347431182861), ('사물', 0.8012840151786804), ('맛있', 0.7991383075714111)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Print the most similar words\")\n",
    "word = input(\"word: \")\n",
    "\n",
    "no_problem = True\n",
    "vocab = model.wv.index_to_key\n",
    "\n",
    "if word not in vocab:\n",
    "    print ('the word ' + word + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "\n",
    "if no_problem:\n",
    "    print(model.wv.most_similar(positive=[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Vector calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the most similar word with the result of [ a - b + c ]\n",
      "a: 서울\n",
      "b: 한국\n",
      "c: 미국\n",
      "most similar word of 서울 - 한국 + 미국 is 종로구 삼성동 세종로\n"
     ]
    }
   ],
   "source": [
    "#word_a = '한국'\n",
    "#word_b = '아시아'\n",
    "#word_c = '유럽'\n",
    "print('Find the most similar word with the result of [ a - b + c ]')\n",
    "word_a = input(\"a: \")\n",
    "word_b = input(\"b: \")\n",
    "word_c = input(\"c: \")\n",
    "\n",
    "# check the words are in the vocabulary\n",
    "no_problem = True\n",
    "vocab = model.wv.index_to_key\n",
    "\n",
    "if word_a not in vocab:\n",
    "    print ('the word ' + word_a + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "\n",
    "if word_b not in vocab:\n",
    "    print ('the word ' + word_b + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "\n",
    "if word_c not in vocab:\n",
    "    print ('the word ' + word_c + ' is not in the vocabulary')\n",
    "    no_problem = False\n",
    "\n",
    "if no_problem:\n",
    "    mostsimilar = model.wv.most_similar(positive=[word_a, word_c], negative=[word_b], topn=5)\n",
    "    print ('most similar word of ' + word_a + ' - ' + word_b + ' + ' + word_c + ' is', mostsimilar[0][0], mostsimilar[1][0], mostsimilar[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
